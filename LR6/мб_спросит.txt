1)Под классификацией будем понимать отнесение объектов (наблюдений, событий) к одному из заранее известных классов.
Классификация - это закономерность, позволяющая делать вывод относительно определения характеристик конкретной группы.
 Таким образом, для проведения классификации должны присутствовать признаки, характеризующие группу,
 к которой принадлежит то или иное событие или объект (обычно при этом на основании анализа уже
 классифицированных событий формулируются правила причисление объекта к группе).
Классификация относится к стратегии обучения с учителем (supervised learning), которое также именуют контролируемым или управляемым обучением.
Классификация может быть одномерной (по одному признаку) и многомерной (по двум и более признакам).

2)Задача кластеризации сходна с задачей классификации, но ее отличие в том, что классы изучаемого набора данных заранее не предопределены и формируются автоматически,
 поэтому синонимами термина "кластеризация" являются "автоматическая классификация", "обучение без учителя" и "таксономия".
Кластеризация предназначена для разбиения совокупности объектов на однородные группы (кластеры или классы).
 Если данные выборки представить, как точки в признаковом пространстве, то задача кластеризации сводится к определению "сгустков точек".
Цель кластеризации - поиск таких сгустков.Кластеризация является описательной процедурой, она не делает никаких статистических выводов,
 но дает возможность провести разведочный анализ и изучить "структуру данных".

3)Наивный Байесовский алгоритм - это алгоритм машинного обучения, который используется для классификации объектов на основе вероятностной модели.
 Он основан на теореме Байеса и предполагает, что каждый признак объекта независим от других признаков.

Алгоритм использует обучающую выборку, которая состоит из объектов, разделенных на классы. 
Для каждого класса алгоритм вычисляет вероятности появления каждого из признаков исходя из класса. 
Затем, когда поступает новый объект для классификации, алгоритм вычисляет вероятность принадлежности объекта каждому из классов на основе 
вычисленных вероятностей признаков. Наиболее вероятный класс становится прогнозируемым классом объекта.

Наивный Байесовский алгоритм работает быстро и может быть эффективным в задачах классификации, особенно когда признаков много, 
и когда они зависимы друг от друга. Однако, его точность может быть снижена, если признаки сильно коррелируют между собой,
 или если в обучающей выборке есть сильные выбросы.

4)Алгоритм классификации Decision Tree работает следующим образом:
1.Выбирается признак, который наиболее эффективно разделяет обучающую выборку на две группы объектов. Для этого используются различные метрики, 
такие как Information Gain, Gini Impurity, Entropy, и другие.
2.Строится узел дерева, соответствующий этому признаку. Если возможно, узел разделяется на два подузла, соответствующие двум значениям признака
(например, "да" и "нет").Если признак категориальный, то узел может быть разделен на подузлы, соответствующие всем его возможным значениям.
3.Для каждого из подузлов повторяются шаги 1-2, пока не будет достигнут критерий останова. Критерий останова может быть определен заранее и может включать в 
себя такие условия, как достижение определенной глубины дерева, или меньшее количество объектов в узле, чем заранее заданное.
4.Когда дерево построено, объекты классифицируются на основе пути, который они проходят через дерево от корня до одного из его листьев.
 Каждый лист соответствует определенному классу.
Классификация Decision Tree является простым и понятным методом, который позволяет строить интерпретируемые модели.
 Кроме того, этот метод позволяет работать с данными, содержащими как числовые, так и категориальные признаки. 
Однако, если дерево слишком глубокое, оно может переобучиться на обучающей выборке и потерять свою способность к обобщению. 

5)Алгоритм Random Forest работает следующим образом:
1.Случайным образом выбирается набор объектов из обучающей выборки, и для каждого объекта случайным образом выбирается набор признаков.
2.Для каждого набора признаков строится решающее дерево на основе выбранных объектов.
3.Процедуры, описанные в пунктах 1 и 2, повторяются многократно, чтобы создать заданное число деревьев.
4.Когда все деревья построены, каждое из них используется для классификации новых объектов. Класс, который получает наибольшее количество голосов от деревьев,
 выбирается как итоговый классификационный результат. 
Алгоритм Random Forest имеет несколько преимуществ по сравнению с классической классификацией деревьев решений, 
так как он уменьшает вероятность переобучения и повышает точность классификации за счет использования ансамбля деревьев.